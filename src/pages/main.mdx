---
title: Human Skin Detection in Color Images
description: Skin detection is the process of discriminating skin and non-skin pixels in an arbitrary image and represents an intermediate step in several image processing tasks, such as facial analysis and biomedical segmentation. Different approaches have been presented in the literature, but a comparison is diffcult to perform due to multiple datasets and varying performance measurements. In this work, the datasets and the state-of-the-art approaches are reviewed and categorized using a new proposed taxonomy. Three different representative skin detector methods of the state of the art are selected and thoroughly analyzed. This approaches are then evaluated on three different state of the art datasets and skin tones sub-datasets using multiple metrics. The evaluation is performed on single and cross dataset scenario to highlight key differences between methods, reporting also the inference time. Finally, the results are organized into multiple tables, using the related figures as an assistance tool to support the discussion. Experimental results demonstrate the strength and weaknesses of each approach, and the need to involve multiple metrics for a fair assessment of the method’s aspects.
---
<!-- hide_table_of_contents: true -->


import { ReactCompareSlider, ReactCompareSliderImage } from 'react-compare-slider';
import Taxonomy from '@site/src/components/Taxonomy';;
import '/src/scripts/index.js'


export const Center = ({children}) => (
  <div style={{
      display: 'flex',
      alignItems: 'center',
      justifyContent: 'center',
  }}>
    {children}
  </div>
)

export const TwoItems = ({one, two}) => (
  <div class="container margin-bottom--xl">
    <div class="row">
      <div class="col col--6">
        {one}
      </div>
      <div class="col col--6">
        {two}
      </div>
    </div>
  </div>
)

export const Caption = ({children}) => (
  <p class="text--center" style={{color: 'var(--ifm-color-gray-600)'}}>{children}</p>
)

export const section = "container margin-bottom--xl"


<div class="container">

<div class={section} >

## Introduction 

  <h1 class="text--center">Human Skin Detection in Color Images</h1>
  <div class="msubtitle">
    <p class="hero__subtitle text--center" style={{color: 'var(--ifm-color-gray-600)'}}>Thesis Overview</p>
    <a class="button button--primary" href="#url">See full thesis</a>
  </div>
  <p class="text--center margin-top--lg">
  The purpose of the thesis is to present a review of the <b>human
  skin detection</b> datasets and approaches of the state of the
  art, and then perform a comparative in-depth analysis of the
  most relevant methods on different databases.
  </p>
</div>



<!-- TODO: change page title!!>

<!--div class="margin-vert--lg" style={{margin: '0 0 var(--ifm-paragraph-margin-bottom)'}}-->
<div class="margin-vert--lg">
  <Center>
    <ReactCompareSlider
      itemOne={<ReactCompareSliderImage src="/img/skin_det_ori3.jpg" alt="Original image" />}
      itemTwo={<ReactCompareSliderImage src="/img/skin_det_red3.png" alt="Detected skin pixels" />}
      style={{
      display: "flex",
      width: "450px",
      maxWidth: "90vw",
      height: "auto"
      }}
    />
  </Center>
</div>


<p>
Skin detection is the process of <b>discriminating skin and non-skin pixels</b>. It is
quite a challenging process because of the large color diversity that objects and
human skin can assume, and scene properties such as lighting and background.
</p>


<!-- TODO: refactor code (follow a react quick tutorial)
<TwoItems
  one={<p>asdasd</p>}
  two={<p>asdasd</p>} >
</TwoItems>
-->


<div class={section} >
  <div class="row">
    <div class="col col--4 margin-top--md">
      <h3 style={{textAlign: 'center'}} >Applications</h3>
      <div class="list-wrap" >
        <ul >
          <li><b>Facial Analysis</b></li>
          <li>Gesture Analysis</li>
          <li><b>Biomedical</b></li>
          <li>Video Surveillance</li>
          <li>Content Filter</li>
          <li><b>Advertisement</b></li>
        </ul>
      </div>
    </div>
    <div class="col col--8">
      <div style={{display: 'flex', alignItems: 'flex-start', justifyContent: 'space-evenly', alignItems: 'center'}}>
        <div style={{zIndex: '-1', display: 'flex', flexWrap: 'nowrap', flexDirection: 'column', justifyContent: 'space-around'}} >
          <img src="/img/applications_facial_analysis.png" style={{maxWidth: '90%', paddingBottom: '1.5rem', maxHeight: '25vh'}} />
          <img src="/img/applications_ads.png" style={{maxWidth: '90%', maxHeight: '25vh'}} />
        </div>
        <img src="/img/applications_biomedical.png" style={{maxWidth: '50%', maxHeight: '40vh'}} />
      </div>
    </div>
  </div>
  <div class="row margin-top--xl reverse-row" >
    <div class="col col--6">
      <div class="row" style={{flexWrap: 'nowrap', justifyContent: 'center'}} >
        <img src="/img/clay.png" style={{maxHeight: '20vh', padding: '0.5rem'}} />
        <img src="/img/wood.jpg" style={{maxHeight: '20vh', padding: '0.5rem'}} />
      </div>
      <div class="row" style={{flexWrap: 'nowrap', justifyContent: 'center'}}>
        <img src="/img/lighting_1.jpg" style={{maxHeight: '20vh', padding: '0.5rem'}} />
        <img src="/img/lighting_2.png" style={{maxHeight: '20vh', padding: '0.5rem'}} />
      </div>
    </div>
    <div class="col col--6">
      <h3 style={{textAlign: 'center'}}>Limitations</h3>
      <div class="list-wrap">
        <ul>
          <li>Materials with <b>skin-like colors</b></li>
          <li>Wide range of <b>skin tones</b></li>
          <li><b>Illumination</b></li>
          <li>Cameras color science</li>
        </ul>
      </div>
    </div>
  </div>
</div>




<div class={section} >

  ## Methodological Approach

  <Center>
    <div class="invert margin-top--lg">
      <img class="svg" src="/img/methodological-approach-vert.svg" alt="Methodological approach flowchart" style={{margin: "1rem 0 var(--ifm-paragraph-margin-bottom)"}} />
    </div>
  </Center>
</div>


<!-- TODO: cite thresholding approach ? -->
<div class={section} >

  ## Taxonomy

  <p>
  Skin detection is a <b>binary classification problem</b>: the pixels of an image must be divided between skin and non-skin classes.
  </p>
  <p>
  One of several ways to categorize methods is to group them according to how the pixel classification is done.
  </p>

  <div id="taxonomy-container" class="container scrolling-section" >
    <div>
      <Taxonomy />
    </div>
    <div class="row">
      <div class="col col--8 col--offset-2">
        <div id="taxonomy-thresholding">
          <p>
          <b>Thresholding</b> approaches use <em>plain rules</em> to classify each pixel as either skin or non-skin. An example is the following.<br />
          A given (Y,Cb,Cr) pixel is a skin pixel if 133&lt;=Cr&lt;=173 and 77&lt;=Cb&lt;=127
          </p>
        </div>
        <div id="taxonomy-fuzzylogic">
          <p>
          <b>Fuzzy logic</b> approaches use a <em>set of rules</em> to calculate a combined truth value between 0 and 1. The truth value drives the classification. 
          </p>
        </div>
        <div id="taxonomy-deeplearning">
          <p>
          <b>Deep learning</b> approaches use training data to create a <em>neural network model</em> which is then used to perform classification.
          </p>
        </div>
        <div id="taxonomy-statistical">
          <p>
          <b>Statistical</b> approaches use training data to create a <em>statistical model</em> which is then used alongside probability calculus to perform classification.
          </p>
        </div>
        <div id="taxonomy-ensemble">
          <p>
          <b>Ensemble</b> approaches use the classifications from different <em>independent</em> machine learning <em>models</em> trained on the same data as votes for determining the best
          classification.
          </p>
        </div>
        <div id="taxonomy-hybrid">
          <p>
          <b>Hybrid</b> approaches make use of different classification techniques that <em>work together</em> to perform the final classification.
          </p>
        </div>
      </div>
    </div>
  </div>
</div>



<!-- h3 class="margin-bottom--lg">Dynamic Thresholding</h3 -->
<!-- TODO: refactor as React component -->
<div class={section} >

  ## Dynamic Thresholding

  <div class="row margin-top--lg">
    <div class="col col--6">
      <Center>
        <div>
          <h3 class="no-bold margin-left--md">Algorithm Overview</h3>
          <ol class="spacing">
            <li>Input image RGB to YCbCr</li>
            <li>Cr<sub>max</sub> Cb<sub>min</sub> computation</li>
            <li>Pixel-wise computation of the correlation rules parameters</li>
            <li>Pixel-wise correlation rules check</li>
          </ol>
        </div>
      </Center>
    </div>
    <div class="col col--6">
      <Center>
        <div class="invert">
          <img src="/img/trapezia_params.png" style={{width: "100%", maxWidth: "450px"}} />
          <Caption>Brancati et al. 2017 [3]</Caption>
        </div>
      </Center>
    </div>
  </div>
  <p>
  The skin pixels clusters assume a trapezoidal shape in the YCb and YCr color subspaces. Moreover, the shape and size of the trapezium vary according to many factors, such as the illumination conditions. In high illumination conditions, the base of the trapezium results larger.
  </p>
  <p>
  Besides, the chrominance components of a skin pixel P with coordinates (P<sub>Y</sub>, P<sub>Cb</sub>, P<sub>Cr</sub>) in the YCbCr space exhibit the following behavior: the further is the (P<sub>Y</sub>, P<sub>Cr</sub>) point from the longer base of the trapezium in the YCr subspace, the further is the (P<sub>Y</sub>, P<sub>Cb</sub>) point from the longer base of the trapezium in the YCb subspace, and vice versa.
  </p>
  <p>
  The aforementioned observations are the base of the method: it tries to define image-specific trapeziums in the YCb and YCr color subspaces and then verifies that the correlation rules between the two subspaces reflect the inversely proportional behavior of the chrominance components.
  </p>
</div>


<div class={section} >

  ## Statistical

  <div class="row">
    <div class="col col-12">
      <div>
        <div class="im-container50 margin-vert--md">
          <img src="/img/skin_det_ori3.jpg" />
          <img src="/img/3d_hist.png" />
        </div>
        <Center>
          <Caption>Example of an image's 3D Histogram</Caption>
        </Center>
      </div>
    </div>
  </div>
  <div class="row margin-top--md">
    <div class="col col--6">
      <Center>
        <div>
          <h3 class="no-bold margin-left--md">Train</h3>
          <ol class="spacing margin-bottom--lg">
            <li>Initialize the skin and non-skin 3D histograms</li>
            <li>Pick (image, mask) from the training set</li>
            <li>Loop every RGB pixel from image</li>
            <li>By checking its mask, the pixel its either skin or non-skin. Add +1 to the relative histogram count at coordinates [r,g,b]</li>
            <li>Return to step 2 until there are images</li>
          </ol>
        </div>
      </Center>
    </div>
    <div class="col col--6">
      <Center>
        <div>
          <h3 class="no-bold margin-left--md">Predict</h3>
          <ol class="spacing margin-bottom--lg">
            <li>Define classifying threshold Θ</li>
            <li>Loop every RGB pixel from input image</li>
            <li>Calculate RGB probability of being skin</li>
            <li>If skin probability > Θ, it is classified as skin</li>
          </ol>
        </div>
      </Center>
    </div>
  </div>
  <p>
  The data is modeled with two 3D histograms representing the probabilities of skin and non-skin classes, and classification is performed via probability calculus by measuring the probability <i>P</i> of each <i>rgb</i> pixel to belong to the <i>skin</i> class:
  </p>
  <Center>
    <div class="invert">
      <img class="svg" src="/img/probability.svg" alt="Probability function" style={{margin: "0 0 var(--ifm-paragraph-margin-bottom)"}} />
    </div>
  </Center>
  <p>
  where <i>s[rgb]</i> is the pixel count contained in bin <i>rgb</i> of the skin histogram and <i>n[rgb]</i> is the equivalent count from the non-skin histogram. <br />
  A particular <i>rgb</i> value is labeled skin if:
  </p>
  <Center>
    <div class="invert">
      <img class="svg" src="/img/probability_thresh.svg" alt="Probability function" style={{margin: "0 0 var(--ifm-paragraph-margin-bottom)"}} />
    </div>
  </Center>
  <p>
  where 0 ≤ Θ ≤ 1 is a threshold value that can be adjusted to trade-off between true positives and false positives.
  </p>
</div>


<!--TODO: srcset with only 3 layers on cell for SVG dense -->
<div class={section}>

  ## U-Net

  <div class="row margin-top--lg">
    <div class="col col--6">
      <Center>
        <div>
          <h3 class="no-bold margin-left--md">Workflow</h3>
          <ol class="spacing">
            <li>Pre-process input image: resize (512×512)px, padding</li>
            <li><em>Extract features</em> in the <b>contracting pathway</b> via convolutions and down-sampling: the spatial information is lost while advanced features are learnt</li>
            <li>Try to <em>retrieve spatial information</em> through up-sampling in the <b>expansive pathway</b> and direct concatenations of <em>dense blocks</em> coming from the contracting pathway</li>
            <li>Provide a final classification map</li>
          </ol>
        </div>
      </Center>
    </div>
    <div class="col col--6">
      <Center>
        <div>
          <img src="/img/skinny_arch.png" style={{width: "100%", maxWidth: "450px"}} />
          <Caption>Tarasiewicz et al. 2020 [5]</Caption>
        </div>
      </Center>
    </div>
  </div>
  <p>
  The Skinny network consists of a modified U-Net incorporating dense blocks and inception modules to benefit from a wider spatial context.
  </p>
  <p>
  An additional deep level is appended to the original U-Net model, to better capture large-scale contextual features in the deepest part of the network.
  The features extracted in the contracting path propagate to the corresponding expansive levels through the dense blocks.
  </p>
  <p>
  The original U-Net convolutional layers are replaced with the inception modules: before each max-pooling layer, in the contracting path, and after concatenating features, in the expanding path.
  Thanks to these architectural choices, Skinny benefits from a wider pixel context.
  </p>
  <div class="container margin-top--lg">
    <div class="im-container padding-small" >
      <img src="/img/inception1.png" class="three-cols" />
      <img src="/img/inception2.png" class="three-cols" />
      <img src="/img/inception3.png" class="three-cols" />
    </div>
    <p>
    The salient content size varies between images. <b>Inception module</b> combines multiple kernels with different sizes for content adaptation.
    </p>
    <Center>
      <div class="invert">
        <img src="/img/dense.svg" style={{padding: "0 0 0.4rem 0"}} />
      </div>
    </Center>
    <p>
    <b>Dense block</b> layers are connected in a way that each one receives feature maps from all preceding layers and passes its feature maps to all subsequent layers.
    </p>
  </div>
</div>


<!-- Bibliography?? TODO-->
<!--
<ol>
   <li id="#nbrancati">N. Brancati, G. De Pietro,M. Frucci, and L. Gallo. “Human skin detection through correlation rules between the YCb and YCr subspaces based on dynamic color clustering”. Computer Vision and Image Understanding 155, 2017, pp. 33–42. <a href="https://doi.org/10.1016/j.cviu.2016.12.001">https://doi.org/10.1016/j.cviu.2016.12.001</a></li>
</ol>

<q cite='#nbrancati'>who would not groan at hearing that Roman knights and senators grovel 
before her (Cleopatra) like eunuchs?</q>
<script>
   var quotes = document.getElementsByTagName('q');
   for (var i in quotes) {
      quotes[i].addEventListener('click', function() { 
         window.location = this.getAttribute('cite'); },
      false);
   }
</script>
-->

</div>
